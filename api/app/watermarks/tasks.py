import dramatiq
from dramatiq.middleware import CurrentMessage
from PIL import ImageOps, Image
from torchvision import transforms
import torch
from typing import List, Dict, Optional
from pathlib import Path
import json
import os
from .const import (
    MODEL_PATHS,
    DEVICE,
    WATERMARKS_DATA_FOLDER,
    WATERMARKS_QUEUE,
    WATERMARKS_RESULTS_FOLDER,
)
from .sources import WatermarkSource

from ..shared.utils.logging import notifying, TLogger, LoggerHelper

FEATURE_TRANSFORMS = lambda sz: transforms.Compose(
    [
        transforms.Resize((sz, sz)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.75, 0.70, 0.65], std=[0.14, 0.15, 0.16]),
    ]
)

MODELS = {}


def auto_load_models(models: List[str] = None):
    global MODELS
    if models is None:
        models = MODEL_PATHS.keys()
    for model in models:
        if model not in MODELS:
            MODELS[model] = torch.load(
                MODEL_PATHS[model], map_location=torch.device(DEVICE)
            ).eval()


@torch.no_grad()
def _detect_watermarks(model: torch.nn.Module, img: Image.Image) -> Dict:
    img = transforms.ToTensor()(img)
    h, w = img.shape[-2:]
    img = img.unsqueeze(0).to(next(model.parameters()).device)

    preds = model(img)

    return {
        "boxes": [
            [x0 / w, y0 / h, x1 / w, y1 / h]
            for x0, y0, x1, y1 in preds[0]["boxes"].cpu().numpy().tolist()
        ],
        "scores": preds[0]["scores"].cpu().numpy().tolist(),
    }


def detect_watermarks(model: torch.nn.Module, img: Image.Image) -> Dict:
    for size in [512, 800, 1400, 2000]:
        im0 = img.copy()
        im0.thumbnail((size, size))
        boxes = _detect_watermarks(model, im0)
        if len(boxes["boxes"]) > 0 and boxes["scores"][0] > 0.3:
            return boxes
    return boxes


@torch.no_grad()
def extract_features(
    model: torch.nn.Module,
    images: List[Image.Image],
    resize_to: int = 352,
    transpositions: List[int | None] | None = None,
) -> torch.Tensor:
    device = next(model.parameters()).device
    imgs = []
    if transpositions is None:
        transpositions = [None]
    tf = FEATURE_TRANSFORMS(resize_to)

    for transp in transpositions:
        for image in images:
            img = image.transpose(transp) if transp else image
            img = tf(img).to(device)
            imgs.append(img)

    imgs = torch.stack(imgs)
    feats = model(imgs)

    return feats.reshape(len(transpositions), len(images), -1)


@torch.no_grad()
def get_closest_matches(
    queries: torch.Tensor, source: WatermarkSource, topk: int = 20, min_sim=0.3
) -> torch.Tensor:
    n_query_flips, n_queries = queries.shape[:2]
    n_compare_flips, n_compare, n_feats = source.features.shape
    queries = torch.nn.functional.normalize(queries, dim=-1)
    sim = torch.mm(
        queries.reshape((-1, n_feats)),
        source.features.to(queries.device, queries.dtype).reshape((-1, n_feats)).T,
    )
    sim = sim.reshape(n_query_flips, n_queries, n_compare_flips, n_compare)
    best_qsim, best_qflip = sim.max(dim=0)
    best_ssim, best_sflip = best_qsim.max(dim=1)
    tops = best_ssim.topk(topk, dim=1)
    return [
        [
            {
                "similarity": ssim.item(),
                "best_source_flip": best_sflip[i, j].item(),
                "best_query_flip": best_qflip[i, best_sflip[i, j], j].item(),
                "query_index": i,
                "source_index": j.item(),
            }
            for (j, ssim) in (zip(tops.indices[i], tops.values[i]))
            if ssim > min_sim
        ]
        for i in range(n_queries)
    ]


@torch.no_grad()
def _pipeline(
    image: Image.Image,
    detect: bool = True,
    compare_to: Optional[WatermarkSource] = None,
) -> Dict:
    to_load = []
    if detect:
        to_load.append("detection")
    if compare_to:
        to_load.append("features")
    auto_load_models(to_load)

    image = ImageOps.exif_transpose(image).convert("RGB")

    crops = [image]
    output = {}

    resize = compare_to.metadata.get("resize", 352) if compare_to else 352

    if detect:
        boxes = detect_watermarks(MODELS["detection"], image)
        output["detection"] = boxes
        crops = []
        if compare_to:
            for box, score in zip(boxes["boxes"], boxes["scores"]):
                if score < 0.5 and len(crops) > 0:
                    break
                box = [
                    box[0] * image.width,
                    box[1] * image.height,
                    box[2] * image.width,
                    box[3] * image.height,
                ]
                cx, cy = (box[0] + box[2]) / 2, (box[1] + box[3]) / 2
                sz = max(box[2] - box[0], box[3] - box[1]) * 1.20
                x0, y0, x1, y1 = (
                    int(cx - sz / 2),
                    int(cy - sz / 2),
                    int(cx + sz / 2),
                    int(cy + sz / 2),
                )
                crops.append(image.crop((x0, y0, x1, y1)).resize((resize, resize)))

    if compare_to and len(crops) > 0:
        feats = extract_features(
            MODELS["features"],
            crops,
            resize,
            [None, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270],
        )
        output["matches"] = get_closest_matches(feats, compare_to)
        output["query_flips"] = [None, "rot90", "rot180", "rot270"]

    print(output)

    return output


@dramatiq.actor(
    time_limit=60000, max_retries=0, store_results=True, queue_name=WATERMARKS_QUEUE
)
@notifying
def pipeline(
    image_path: str,
    detect: bool = True,
    experiment_id: str = "",
    compare_to: Optional[str] = None,
    logger: TLogger = LoggerHelper,
):
    image = Image.open(image_path)
    compare_to = WatermarkSource(compare_to) if compare_to else None
    output = _pipeline(image, detect, compare_to)

    result_dir = WATERMARKS_RESULTS_FOLDER
    result_dir.mkdir(parents=True, exist_ok=True)
    with open(
        result_dir / f"{CurrentMessage.get_current_message().message_id}.json", "w"
    ) as f:
        json.dump(output, f)
    os.unlink(image_path)

    return output
